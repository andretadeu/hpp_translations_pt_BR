1
00:00:05,460 --> 00:00:06,340
Hi everyone.

2
00:00:06,340 --> 00:00:09,600
Welcome back to the Heterogeneous Parallel
Programming class.

3
00:00:11,240 --> 00:00:13,570
In the next several lectures, we're going

4
00:00:13,570 --> 00:00:19,480
to go over several major parallel
computation patterns.

5
00:00:19,480 --> 00:00:22,850
And we're going to introduce these
patterns for two reasons.

6
00:00:22,850 --> 00:00:27,850
One is these patterns are very important
in parallel computing in general.

7
00:00:27,850 --> 00:00:30,540
So these patterns are important of

8
00:00:30,540 --> 00:00:31,310
their own.

9
00:00:31,310 --> 00:00:33,820
And the second one is what each pattern
will

10
00:00:33,820 --> 00:00:38,450
be able to introduce some important
techniques and concepts.

11
00:00:38,450 --> 00:00:41,150
That usually go with these patterns.

12
00:00:41,150 --> 00:00:44,130
And then so, we're going to start with
convolution.

13
00:00:46,660 --> 00:00:49,460
The objective of this lecture is to help
you to learn

14
00:00:49,460 --> 00:00:51,620
convolution, which is an important
parallel

15
00:00:51,620 --> 00:00:54,220
computing pattern, as we already
mentioned.

16
00:00:54,220 --> 00:00:57,570
And this particular computation is widely
used

17
00:00:57,570 --> 00:01:02,080
in signal processing, image processing,
and video processing.

18
00:01:02,080 --> 00:01:05,780
And more importantly, it also serves as

19
00:01:05,780 --> 00:01:09,630
a foundation for a several other
computational patterns.

20
00:01:09,630 --> 00:01:12,460
And then, for example, the stencil
computation

21
00:01:12,460 --> 00:01:16,350
that is used in many science and
engineering applications.

22
00:01:16,350 --> 00:01:21,790
And then along with this pattern, we are
going to introduce important

23
00:01:21,790 --> 00:01:27,990
techniques for tiling data for more
intricate access patterns and also

24
00:01:27,990 --> 00:01:33,650
how we can take advantage of some of the
specialized cache memories in GPUs.

25
00:01:37,370 --> 00:01:39,490
Lets go back to convolution a little bit.

26
00:01:39,490 --> 00:01:44,250
The convolution applications are usually
performed as a filter

27
00:01:44,250 --> 00:01:49,090
that transforms signals or pixels into
more desirable values.

28
00:01:49,090 --> 00:01:54,580
And this is the reason why we often see
convolution in the sig,

29
00:01:54,580 --> 00:02:01,080
like signal processing or image
processing, or video processing.

30
00:02:01,080 --> 00:02:02,950
And some of these filters are

31
00:02:02,950 --> 00:02:06,200
actually used to smooth out the signal
values so

32
00:02:06,200 --> 00:02:09,640
that we can see the big picture more
easily.

33
00:02:09,640 --> 00:02:13,800
And in some other situations we actually
do the opposite.

34
00:02:13,800 --> 00:02:19,470
We are, we use something like Gaussian
filters as a convolution computation

35
00:02:19,470 --> 00:02:23,950
to sharpen the bound, the boundaries and
edges of objects in images.

36
00:02:26,920 --> 00:02:32,280
So what is convolution computation?
Concretely

37
00:02:32,280 --> 00:02:37,550
a convolution computation is an array
operation, where each output data element,

38
00:02:37,550 --> 00:02:42,450
is a weighted sum of a collection of
neighboring input elements.

39
00:02:42,450 --> 00:02:46,690
In general, when we perform convolution,
we will transform an

40
00:02:46,690 --> 00:02:51,370
input array into an output array of the
same size.

41
00:02:51,370 --> 00:02:52,280
And the they

42
00:02:52,280 --> 00:02:55,670
will, there is usually a one-to-one
correspondence between

43
00:02:55,670 --> 00:02:58,760
the input array element and output array
element.

44
00:02:58,760 --> 00:03:03,150
To compute each output array element we
will take the input element in

45
00:03:03,150 --> 00:03:08,350
corresponding input element in the input
array and some of the neighboring elements

46
00:03:08,350 --> 00:03:13,780
in that in that input array to perform a
weighted sum calculation.

47
00:03:13,780 --> 00:03:17,960
And the weights used in this calculation
are defined

48
00:03:17,960 --> 00:03:20,690
as an input mask array.

49
00:03:20,690 --> 00:03:25,350
And this is commonly referred to as
convolution kernel, but

50
00:03:25,350 --> 00:03:30,890
unfortunately in CUDA, kernel also has the
meaning of kernel functions.

51
00:03:30,890 --> 00:03:36,210
So in order to avoid confusion, we're not
going to call, call these

52
00:03:36,210 --> 00:03:41,610
masks convolution kernels but we are going
to call them convolution masks.

53
00:03:41,610 --> 00:03:43,360
And the same

54
00:03:43,360 --> 00:03:47,970
convolution mask is usually used in
calculating all

55
00:03:47,970 --> 00:03:52,070
the output elements in a particular
convolution computation.

56
00:03:53,070 --> 00:03:57,840
So, here is a very simple example of one
dimensional convolution.

57
00:03:57,840 --> 00:04:01,040
And here we show a mask of five elements.

58
00:04:01,040 --> 00:04:05,116
And in order to calculate one output
element, we're

59
00:04:05,116 --> 00:04:08,650
going to take its corresponding input
element, and we're going

60
00:04:08,650 --> 00:04:12,810
to take the mask and align that mask, the
middle,

61
00:04:12,810 --> 00:04:16,780
the center of the mask to the
corresponding input element.

62
00:04:16,780 --> 00:04:22,870
So, in this case, we have we are going to
calculate P[2] in the output.

63
00:04:22,870 --> 00:04:27,840
And the corresponding input is N[2] and
then we are going to take the

64
00:04:27,840 --> 00:04:32,350
center of the mask which is M[2] and we
are going to align M[2] with N[2].

65
00:04:32,350 --> 00:04:33,062
So that,

66
00:04:33,062 --> 00:04:38,950
M[0] will be aligned to N[0], M[1] is
going to align to N[1].

67
00:04:38,950 --> 00:04:40,110
And so on.

68
00:04:40,110 --> 00:04:45,320
And this, after this alignment we're going
to do a pairwise multiplication.

69
00:04:45,320 --> 00:04:51,534
That's how we do the weighted part.
And the pair wise multiplication is going

70
00:04:51,534 --> 00:04:57,960
to give us 1 times 3 which is 3 and then 2
times 4 which is 8 here.

71
00:04:57,960 --> 00:05:03,940
And then 3 times 5 which is 15.
And then 4 times 4 is 16, and so on.

72
00:05:03,940 --> 00:05:07,420
And then, once we have all these mult

73
00:05:07,420 --> 00:05:09,670
products, we are going to add them
together.

74
00:05:09,670 --> 00:05:13,200
So, that's why it's a weighted sum
calculation.

75
00:05:13,200 --> 00:05:15,370
And once we added up all these five

76
00:05:15,370 --> 00:05:20,280
values, they become the output value, 57,
in P[2].

77
00:05:21,410 --> 00:05:23,400
So the

78
00:05:23,400 --> 00:05:29,730
in general we would, we often use
fractions for the mask values.

79
00:05:29,730 --> 00:05:31,660
So that we don't just produce bigger and

80
00:05:31,660 --> 00:05:34,500
bigger values as we calculate the weighted
sum.

81
00:05:34,500 --> 00:05:38,940
However, for this particular example for
simplicity, we'll use integer values.

82
00:05:38,940 --> 00:05:45,710
Just so that, that is easy for you to see
that, the computation pattern.

83
00:05:45,710 --> 00:05:49,230
When we calculate the next element P[3]

84
00:05:49,230 --> 00:05:53,365
we again use the same mask.
However, now the center of the mask is

85
00:05:53,365 --> 00:05:57,031
not aligned to the corresponding input
element N[3].

86
00:05:58,090 --> 00:06:02,935
So now we are aligning N[1], with M[0],
and

87
00:06:02,935 --> 00:06:08,210
N[2] with M[1], and so on.
And we're still going to be calculate

88
00:06:08,210 --> 00:06:14,350
doing the same weighted sum calculation.
So we have 2 times 3, which is

89
00:06:14,350 --> 00:06:18,570
6 here.
And then 3 times 4, which is 12 here.

90
00:06:18,570 --> 00:06:21,270
And then, we finish all the calculation.

91
00:06:21,270 --> 00:06:27,280
We, we sum up all the products into the
answer for P[3].

92
00:06:27,280 --> 00:06:33,570
So, now as you can see, by taking the same
mask and shifting the mask through

93
00:06:33,570 --> 00:06:39,670
each input element, we will be able to
produce each output element as we

94
00:06:39,670 --> 00:06:41,850
continue this computation.

95
00:06:41,850 --> 00:06:48,670
And you can also see that because the
calculation of P[2], P[3], and so on

96
00:06:48,670 --> 00:06:54,290
are independent of each other, so this is
intrinsically a very parallel computation.

97
00:06:56,790 --> 00:06:59,880
Convolution also has boundary conditions.

98
00:06:59,880 --> 00:07:05,170
When we calculate a output element that is
close to the beginning or end of

99
00:07:05,170 --> 00:07:10,670
the output array, we are going to face
some boundary conditions.

100
00:07:10,670 --> 00:07:14,080
So for example, when we calculate P[1] we

101
00:07:14,080 --> 00:07:18,270
need to have two input elements to the
left.

102
00:07:18,270 --> 00:07:22,280
Unfortunately, we'll only have one input
element in the valid range.

103
00:07:22,280 --> 00:07:27,485
So, we will have, we'll essentially have
one of the mask elements to pair up with a

104
00:07:27,485 --> 00:07:33,310
non-existing element.
So this is what we call the ghost element.

105
00:07:33,310 --> 00:07:37,450
And this element does not exist in the
input, however

106
00:07:37,450 --> 00:07:41,220
when we calculate one of the outputs that
are close

107
00:07:41,220 --> 00:07:43,880
to the boundary, we are going to need have
some

108
00:07:43,880 --> 00:07:48,230
kind of policy to determine the value of
these non-existing

109
00:07:48,230 --> 00:07:52,860
elements, and that there are various
policies that we can use.

110
00:07:52,860 --> 00:07:58,150
For example, we can just say oh, all the
non-existing elements will have

111
00:07:58,150 --> 00:08:01,840
zero value which is what we are going to
use in the lab assignments.

112
00:08:01,840 --> 00:08:08,000
But in some of the applications they also
they could have a policy where we will

113
00:08:08,000 --> 00:08:14,290
say okay, all the non-existing elements
will assume exactly the same value as

114
00:08:14,290 --> 00:08:16,520
the P[0].

115
00:08:16,520 --> 00:08:20,470
So, that's also a valid policy and
depending on the

116
00:08:20,470 --> 00:08:26,260
application we can have different policies
for determining these ghost elements.

117
00:08:26,260 --> 00:08:30,010
And so in, as I mentioned in our labs
we're going to just

118
00:08:30,010 --> 00:08:33,060
assume that all the non-existing elements
will

119
00:08:33,060 --> 00:08:36,100
assume a value zero in our calculations.

120
00:08:38,510 --> 00:08:45,410
So here is a simple kernel that does 1D
convolution in CUDA.

121
00:08:45,410 --> 00:08:50,440
And we're going to, you know, to, to take
several inputs.

122
00:08:50,440 --> 00:08:56,450
The input array N, and then the mask array
M.

123
00:08:56,450 --> 00:09:00,170
And we're going to have a the, the mask

124
00:09:00,170 --> 00:09:03,700
width and that which is the number of
elements

125
00:09:03,700 --> 00:09:06,320
in a mask and then the width, which is

126
00:09:06,320 --> 00:09:10,310
the element the number of elements in the
input array.

127
00:09:10,310 --> 00:09:15,480
Of course, we will need to have the
output, a pointer to the output array P.

128
00:09:15,480 --> 00:09:20,960
So this is a very familiar expression for
you

129
00:09:20,960 --> 00:09:25,540
which assigns one thread to each of the
output elements.

130
00:09:25,540 --> 00:09:28,970
So this, this is a very familiar
expression.

131
00:09:28,970 --> 00:09:32,420
And then for the particular output element

132
00:09:32,420 --> 00:09:35,730
we're going to initialize that value to
zero.

133
00:09:35,730 --> 00:09:40,640
And the remember that the, in the input
array well, we need to

134
00:09:40,640 --> 00:09:45,910
have several neighbor elements so that the
beginning of the input

135
00:09:45,910 --> 00:09:50,910
neighborhood that we'll be using for
calculating a particular element us half

136
00:09:50,910 --> 00:09:56,100
of the mask to the left for 1D
calculation.

137
00:09:56,100 --> 00:10:01,225
So in our previous slide we see that when
we calculate

138
00:10:01,225 --> 00:10:06,240
P[3], we need to have N[1] as the
beginning of the neighborhood.

139
00:10:06,240 --> 00:10:09,390
So we're we're taking the mask that width

140
00:10:09,390 --> 00:10:12,910
which is five Divided by two, that gives
us

141
00:10:12,910 --> 00:10:15,190
half the width, because it's an odd number

142
00:10:15,190 --> 00:10:20,540
and the c integer division is is, will
trunc,

143
00:10:20,540 --> 00:10:25,390
truncate the output value.
So we will get a value, two.

144
00:10:25,390 --> 00:10:32,640
So this gives us the element that is two
before the corresponding input element.

145
00:10:32,640 --> 00:10:36,980
So this is how we calculate that input
starting point.

146
00:10:36,980 --> 00:10:39,480
The starting point is the beginning of the
neighborhood

147
00:10:39,480 --> 00:10:42,730
that we are using for the weighted sum
calculation.

148
00:10:42,730 --> 00:10:45,780
So once you understand the input

149
00:10:45,780 --> 00:10:50,930
point variable we can go into this this
neighborhood to

150
00:10:50,930 --> 00:10:56,670
the sum of, weighted sum calculation by
looping through

151
00:10:56,670 --> 00:11:02,150
all the the mask elements and their
corresponding input elements.

152
00:11:02,150 --> 00:11:05,339
So we would start with N starting point
and N

153
00:11:05,339 --> 00:11:10,530
will start with element zero would be
doing a pairwise product.

154
00:11:10,530 --> 00:11:11,060
So that's

155
00:11:11,060 --> 00:11:12,611
the j component.

156
00:11:12,611 --> 00:11:19,758
Once we do the pairwise multiplication, we
accumulate the product into the P value.

157
00:11:19,758 --> 00:11:24,980
Once we have gone through the entire mask
width,

158
00:11:24,980 --> 00:11:27,740
then we have calculated the weighted sum
of that neighborhood.

159
00:11:28,890 --> 00:11:32,500
During the calculation, we also need to be
careful

160
00:11:32,500 --> 00:11:36,500
that the starting point that we use is
actually

161
00:11:36,500 --> 00:11:38,430
you know, well within the valid range.

162
00:11:38,430 --> 00:11:42,042
So, when, we're taking a starting point
plus J and

163
00:11:42,042 --> 00:11:45,018
we'll see if it's greater than or equal to
0.

164
00:11:45,018 --> 00:11:48,560
And if it is not greater than or equal 0
we're not going to

165
00:11:48,560 --> 00:11:53,100
do the calculation which means that we
assume that the ghost element is 0.

166
00:11:53,100 --> 00:11:57,340
When we have the 0, that ghost element is
not going to effect the weighted sum.

167
00:11:57,340 --> 00:12:00,140
So we don't need to do this accumulation.

168
00:12:00,140 --> 00:12:02,140
The same test is

169
00:12:02,140 --> 00:12:03,830
also done on the right hand side.

170
00:12:03,830 --> 00:12:07,330
That is that when the N start point plus
j.

171
00:12:07,330 --> 00:12:10,000
The input element that we are using for

172
00:12:10,000 --> 00:12:12,970
this weighted sum is greater than or equal
to

173
00:12:12,970 --> 00:12:16,380
the width, then we we also assume that

174
00:12:16,380 --> 00:12:20,160
they are, these ghost elements are of 0
value.

175
00:12:20,160 --> 00:12:26,760
So, we are going to skip this accumulation
step by assuming that those values are 0.

176
00:12:26,760 --> 00:12:27,240
So this

177
00:12:27,240 --> 00:12:32,920
particular for loop with the, with the
conditional if test essentially implements

178
00:12:32,920 --> 00:12:38,860
a policy that all the ghost element
outside the valid range all have 0 value.

179
00:12:38,860 --> 00:12:42,340
Once we've finished the entire weighted
sum calculation, we

180
00:12:42,340 --> 00:12:46,000
have the answer for the output element in
P value.

181
00:12:46,000 --> 00:12:48,290
So now we can write P value

182
00:12:48,290 --> 00:12:52,370
into the corresponding position in the
output array.

183
00:12:55,830 --> 00:12:59,560
Now that you understand the 1D
convolution, 2D

184
00:12:59,560 --> 00:13:04,730
convolution is a very straightforward
generalization of 1D convolution.

185
00:13:04,730 --> 00:13:12,920
So we have an a, output in of 2D array and
it's calculated

186
00:13:12,920 --> 00:13:18,330
based on the corresponding element in the
two dimensional input array.

187
00:13:18,330 --> 00:13:21,030
And whenever we calculate an output, we
take

188
00:13:21,030 --> 00:13:26,130
the corresponding input element and we
have a two dimensional neighborhood.

189
00:13:26,130 --> 00:13:30,840
That we are going to we are going to use
to, and the mask

190
00:13:30,840 --> 00:13:35,900
is now also going to be a two dimensional
array that defines the neighborhood.

191
00:13:35,900 --> 00:13:41,290
We will be again using pairwise
multiplication so 1 times

192
00:13:41,290 --> 00:13:46,460
1 is 1 here and then 2 times 2 is 4 so we
see the element

193
00:13:46,460 --> 00:13:48,580
4 in the product.

194
00:13:48,580 --> 00:13:54,360
So this is a pairwise output of all the
elements involved.

195
00:13:54,360 --> 00:13:55,590
And then we do a sum.

196
00:13:55,590 --> 00:14:00,430
We simply do a a sum of this of this
product array,

197
00:14:00,430 --> 00:14:05,450
into the final answer.
So again we see that this, the output

198
00:14:05,450 --> 00:14:11,590
element is simply a weighted sum of all
the elements in the defined

199
00:14:11,590 --> 00:14:14,840
neighborhood of this corresponding input
element.

200
00:14:17,830 --> 00:14:20,380
Just as in the 1D convolution, a

201
00:14:20,380 --> 00:14:24,040
2D convolution can also have boundary
conditions.

202
00:14:24,040 --> 00:14:30,910
When we calculate a output element that is
close to the edges of the array

203
00:14:30,910 --> 00:14:33,210
we could, we can have a situation where

204
00:14:33,210 --> 00:14:37,570
the neighborhood will extend beyond the
valid input.

205
00:14:37,570 --> 00:14:43,190
So in this case we also assume that all
the ghost elements are all value 0.

206
00:14:43,190 --> 00:14:47,020
So when you see the when you write a 2D
convolution

207
00:14:47,020 --> 00:14:51,080
kernel you should have a similar condition
test in that for loop.

208
00:14:51,080 --> 00:14:55,380
It's just that the for loop should be now
a two dimensional for loop that goes

209
00:14:55,380 --> 00:15:01,132
through both the X and Y dimensions, to to
calculate a weighted sum of a two

210
00:15:01,132 --> 00:15:06,900
dimensional area rather than just a one
dimensional area.

211
00:15:06,900 --> 00:15:08,952
So this concludes the introduction

212
00:15:08,952 --> 00:15:14,400
to convolution computation.
And then with this, if you are interested

213
00:15:14,400 --> 00:15:19,140
in learning more about the convolution
computation I would like to

214
00:15:19,140 --> 00:15:24,380
encourage you to read the textbook
sections 8.1 and 8.2.

215
00:15:24,380 --> 00:15:25,697
Thank you.

