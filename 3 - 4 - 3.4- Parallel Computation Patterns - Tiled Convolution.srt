1
00:00:00,826 --> 00:00:05,299
[MUSIC]

2
00:00:05,299 --> 00:00:09,556
Hi everyone, welcome back to heterogeneous
parallel programming class.

3
00:00:09,556 --> 00:00:15,609
We're at lecture 3.4 and we'll be
introducing tiled convolution computation.

4
00:00:17,110 --> 00:00:22,346
The objective of this lecture is for you
to learn about in how we can introduce

5
00:00:22,346 --> 00:00:27,644
tiled techniques data [UNKNOWN] techniques
into convolution algorithms.

6
00:00:27,644 --> 00:00:31,070
And there're some intricate fact aspects

7
00:00:31,070 --> 00:00:37,140
of convolution that affects the complexity
and efficiency of tiling algorithms.

8
00:00:37,140 --> 00:00:41,740
And in particular we're going to be
designing the input

9
00:00:41,740 --> 00:00:46,240
tiles and output tiles differently, in
order to manage their complexity.

10
00:00:49,890 --> 00:00:55,290
So this slide shows the design of output
tiles and output data index

11
00:00:55,290 --> 00:01:00,410
mapping for a type one dimensional tile
convolution algorithm.

12
00:01:00,410 --> 00:01:07,020
So here we assume that we are going to
divide the output array into output tiles.

13
00:01:07,020 --> 00:01:11,376
And each tile is going to be generated by one
thread

14
00:01:11,376 --> 00:01:15,336
block and we are going to define the the

15
00:01:15,336 --> 00:01:17,712
size of the output file with a

16
00:01:17,712 --> 00:01:22,898
compiled-time constant O_TILE_WIDTH, output
tile width.

17
00:01:22,898 --> 00:01:29,131
And each thread block like as I mentioned
will calculate a, an output tile.

18
00:01:29,131 --> 00:01:34,651
And then we, based on this its actually
very similar to vector

19
00:01:34,651 --> 00:01:40,355
addition, and we will use this familiar
expression to, to make

20
00:01:40,355 --> 00:01:46,151
sure that every the element is being
generated by one thread in the

21
00:01:46,151 --> 00:01:51,990
in the thread block.
So, we'll be calculating the, output

22
00:01:51,990 --> 00:01:57,640
array index with blockidx.x times

23
00:01:57,640 --> 00:02:02,410
output tile width plus thread index idx.x.

24
00:02:02,410 --> 00:02:06,360
So essentially, we're going to be using
the blockIdx.x

25
00:02:06,360 --> 00:02:10,440
to skip over the all the output tiles

26
00:02:10,440 --> 00:02:13,880
that are being covered by previous thread
blocks.

27
00:02:13,880 --> 00:02:15,890
And then within the thread block, we're
going to be

28
00:02:15,890 --> 00:02:20,240
using threadIdx to select one of the
output elements.

29
00:02:20,240 --> 00:02:25,850
So if for example, if we look at thread
one, of

30
00:02:25,850 --> 00:02:31,970
you know, block one blockIdx is 1, so
we're going to to

31
00:02:31,970 --> 00:02:37,682
have 1 times 4 that was skip over all the
four elements in this

32
00:02:37,682 --> 00:02:43,610
in this tile and threadIdx is 1.
So we are going to add 1 to 4 which is 5.

33
00:02:43,610 --> 00:02:47,532
So that allows to thread one of block one

34
00:02:47,532 --> 00:02:53,310
to to be, to write into output array
element five.

35
00:02:53,310 --> 00:02:57,410
So in this particular example, we are
using a very

36
00:02:57,410 --> 00:03:04,160
small output tile width which is 4 and in
practice, we will be using

37
00:03:04,160 --> 00:03:10,219
hundreds or even a thousand also as the
output dat, output tile size.

38
00:03:11,620 --> 00:03:17,030
This slide introduces the concept of input
tiles and we'll define

39
00:03:17,030 --> 00:03:22,512
the input tile as all the elements that
are needed to calculate the elements of

40
00:03:22,512 --> 00:03:23,790
output tile.

41
00:03:23,790 --> 00:03:27,890
So the input tile is really defined
relative to an output tile.

42
00:03:27,890 --> 00:03:33,362
So remember when we have calculated
convolution value for

43
00:03:33,362 --> 00:03:38,450
element 4, if we use a mask size of five,
we are going to

44
00:03:38,450 --> 00:03:43,154
need to have elements 2, 3, 4, 5, 6, and
so,

45
00:03:43,154 --> 00:03:48,050
the input tile is actually you know you
know depends

46
00:03:48,050 --> 00:03:53,750
on the the the mask width.
So the longer

47
00:03:53,750 --> 00:04:00,410
the mask the more the input elements will
be relative to an output tile that.

48
00:04:00,410 --> 00:04:05,730
So in this particular case, we will need
to have, two elements, additional elements

49
00:04:05,730 --> 00:04:11,560
on each side in order to be able to
generate all the output tile elements.

50
00:04:11,560 --> 00:04:13,320
So we were in this

51
00:04:13,320 --> 00:04:18,718
particular example, the output tile width
is 4 and the input tile width is 8.

52
00:04:18,718 --> 00:04:24,262
In practice, well, we have much, much
bigger output tiles the mask

53
00:04:24,262 --> 00:04:29,070
is in general not going to be a whole lot
bigger than then 5.

54
00:04:29,070 --> 00:04:34,180
So, we will actually have output tile,
input tile relatively close to

55
00:04:34,180 --> 00:04:38,500
each other when the output tile size is
is, is much bigger

56
00:04:38,500 --> 00:04:43,275
in practice.
So, but in this example, we would have,

57
00:04:43,275 --> 00:04:48,880
you know it's, it looks like the input
tile is twice the size of the output tile.

58
00:04:48,880 --> 00:04:53,930
But in practice they will be much closer
in size relatively close to

59
00:04:53,930 --> 00:04:59,220
each other in size.
And we are going to set the

60
00:04:59,220 --> 00:05:04,245
block dimension so that we have, we are
going to have enough

61
00:05:04,245 --> 00:05:08,090
threads to cover all the input tile
elements.

62
00:05:08,090 --> 00:05:12,850
So in this particular case, instead of
setting the blockDim to be 4,

63
00:05:12,850 --> 00:05:17,660
we'll actually set the blockDim to be 8,
just so that we'll have one

64
00:05:17,660 --> 00:05:23,102
thread to load every input element and
then when we, when it comes to

65
00:05:23,102 --> 00:05:28,570
calculating the output element some of the
threads will be put, put to idle.

66
00:05:28,570 --> 00:05:29,730
So that only

67
00:05:29,730 --> 00:05:35,390
some of the threads will be involving
actually calculating output elements.

68
00:05:39,330 --> 00:05:44,410
So that leads to the policy for
determining the

69
00:05:44,410 --> 00:05:49,650
input data index.
So essentially we will have thread zero of

70
00:05:49,650 --> 00:05:55,040
every thread block to load the zeroes
element of the input tile and so on.

71
00:05:55,040 --> 00:05:59,050
So this essentially allows us to use all
the threads in

72
00:05:59,050 --> 00:06:05,060
the thread block to load consecutive
elements in the input tile.

73
00:06:05,060 --> 00:06:12,220
And as you can see because of the the
shifting of the element

74
00:06:12,220 --> 00:06:17,370
whenever when thread zero loads the output
element, let's

75
00:06:17,370 --> 00:06:22,393
say index_o, we will need to subtract the
radius.

76
00:06:22,393 --> 00:06:28,250
Essentially the radius of the the mask
which is two in this example.

77
00:06:28,250 --> 00:06:30,625
So that we can derive

78
00:06:30,625 --> 00:06:35,600
the index for to use in loading the input
element.

79
00:06:35,600 --> 00:06:40,380
So thread 0 will be loading input element
two

80
00:06:40,380 --> 00:06:44,930
whereas, it's going to be calculating
output element 4.

81
00:06:44,930 --> 00:06:50,324
And so this is a important relation in the
in this particular

82
00:06:50,324 --> 00:06:55,532
design the input index is going to be the
output index minus n,

83
00:06:55,532 --> 00:06:59,531
which n is the mask width mi divided by 2,

84
00:06:59,531 --> 00:07:03,800
which will be we'll call the radius of the
mask.

85
00:07:08,010 --> 00:07:12,380
So here is the a simple piece of code that
allows all

86
00:07:12,380 --> 00:07:17,620
the threads to collaborate and load a
input tile in the thread block.

87
00:07:17,620 --> 00:07:22,660
So, we are going to test whether the input
index is within the valid range.

88
00:07:22,660 --> 00:07:27,430
So if the index, the input index is within
a, the valid range,

89
00:07:27,430 --> 00:07:32,560
we are going to use that index to fetch
the n element into a,

90
00:07:32,560 --> 00:07:36,111
corresponding shared memory location.

91
00:07:36,111 --> 00:07:40,199
And then obviously, we can just use the
thread index to select

92
00:07:40,199 --> 00:07:45,120
to, to select one of the shared memory
array locations for this purpose.

93
00:07:45,120 --> 00:07:52,320
And then if the if the in, input index is
outside of value range we would

94
00:07:52,320 --> 00:07:58,768
just assign a zero value into the sh,
corresponding shared memory location.

95
00:07:58,768 --> 00:08:04,350
And this implements the policy of treating
all the ghost elements as zeros.

96
00:08:06,560 --> 00:08:12,100
After this load you will need to remember
to do a barrier synchronization to make

97
00:08:12,100 --> 00:08:13,950
sure that all the threads in a thread

98
00:08:13,950 --> 00:08:18,610
block have completed their their, their
loading activity.

99
00:08:18,610 --> 00:08:22,093
So then we can transition into the
calculation

100
00:08:22,093 --> 00:08:26,820
of output elements out of the shared
memory contents.

101
00:08:26,820 --> 00:08:32,190
So, you know, remember that, with, we
meant, we have more elements

102
00:08:32,190 --> 00:08:37,400
in the thread block, for, covering the
output tile, the the

103
00:08:37,400 --> 00:08:41,120
size of the thread block is actually
bigger than the output tile.

104
00:08:41,120 --> 00:08:46,600
So, we're only going to be using the first
output tile with element threads to

105
00:08:46,600 --> 00:08:51,490
calculate the output tile and the rest of
the threads will be put into idle.

106
00:08:51,490 --> 00:08:57,967
So this is implemented that with this if
statement and only the threads whose

107
00:08:57,967 --> 00:09:00,520
the index is less than the output

108
00:09:00,520 --> 00:09:03,750
tile width will participate in this
calculation.

109
00:09:03,750 --> 00:09:08,910
And for each of these threads we're going
to initialize the output value to zero and

110
00:09:08,910 --> 00:09:14,320
then we're going to go through the loop to
go go through the neighborhood.

111
00:09:14,320 --> 00:09:17,600
And for each value in the neighborhood
we're going

112
00:09:17,600 --> 00:09:23,200
to take the M value and then a
corresponding,

113
00:09:24,498 --> 00:09:28,110
shared memory n value to do the product

114
00:09:28,110 --> 00:09:31,520
and then we would accumulate in this for
loop.

115
00:09:31,520 --> 00:09:37,410
So this calculation shows that for
accessing the shared memory

116
00:09:37,410 --> 00:09:43,050
we're going to have a starting location
defined by threadIdx.x.

117
00:09:43,050 --> 00:09:48,090
And this is actually illustrated about two
slide, slides ago.

118
00:09:48,090 --> 00:09:49,629
And let's go back

119
00:09:49,629 --> 00:09:55,070
to this definition here.
So when we load the

120
00:09:55,070 --> 00:10:00,490
input tile into the shared memory, we're
going we're going to have all the

121
00:10:00,490 --> 00:10:06,600
threads, to to, to set their starting
point with their thread index.

122
00:10:06,600 --> 00:10:12,540
So thread zero is going to start with
shared memory locate, array location zero.

123
00:10:12,540 --> 00:10:14,844
So thread zero is going to be accessing 2,

124
00:10:14,844 --> 00:10:17,119
3, 4, 5, 6 as at the shared memory.

125
00:10:17,119 --> 00:10:20,080
And thread one is going to start with
element one.

126
00:10:20,080 --> 00:10:22,090
So that's why we are using the thread

127
00:10:22,090 --> 00:10:25,700
index as this beginning location in that
for loop.

128
00:10:28,570 --> 00:10:31,320
So we, so all these threads, we have used
the thread

129
00:10:31,320 --> 00:10:34,420
index as the beginning location and then
we will have the

130
00:10:34,420 --> 00:10:38,880
j to increment through the neighborhood
for both m and n,

131
00:10:38,880 --> 00:10:42,250
so that we can calculate those weighted
sums in the neighborhood.

132
00:10:42,250 --> 00:10:45,350
And once we finish the for loop, we take
the output value,

133
00:10:45,350 --> 00:10:50,070
we assign that to the output array P and
then we're done.

134
00:10:50,070 --> 00:10:53,720
So it's important to remember that only

135
00:10:53,720 --> 00:10:57,710
the thread zero through output tile width
minus

136
00:10:57,710 --> 00:11:01,170
one will participate in the calculation of
this output.

137
00:11:04,350 --> 00:11:07,690
So that's a design of a one dimensional
tile

138
00:11:07,690 --> 00:11:12,710
kernel and then we need also set the block
size.

139
00:11:12,710 --> 00:11:17,075
But remember the block size is set so that
we have enough threads in

140
00:11:17,075 --> 00:11:21,970
a block to load for every, for each thread
to load a input tile element.

141
00:11:21,970 --> 00:11:29,430
So let's say if we want to have 100 1024
threads in a thread block, then the input

142
00:11:29,430 --> 00:11:34,250
tile size is going to be 1024 and then the
output tile width really needs

143
00:11:34,250 --> 00:11:39,220
to be you know, what n over 2, n, n

144
00:11:39,220 --> 00:11:45,100
minus 1, not n minus 1, but mask width
minus one

145
00:11:45,100 --> 00:11:50,130
smaller than the the input tiles size.
Because with, the input

146
00:11:50,130 --> 00:11:54,890
tile has you know, what the mask width
divide by

147
00:11:54,890 --> 00:11:57,970
two, which is the radius of the mask on
each side.

148
00:11:57,970 --> 00:12:01,220
So, we're, for a mask value of five, we're
going to

149
00:12:01,220 --> 00:12:04,770
have two elements on each side for the
input tile.

150
00:12:04,770 --> 00:12:10,440
So, in general, we are going to, if n is
mask o, over 2 we are going to

151
00:12:10,440 --> 00:12:16,350
have you know what mask width minus 1
additional

152
00:12:16,350 --> 00:12:19,670
element in the input tile compared to the
output tile.

153
00:12:19,670 --> 00:12:24,480
So if we want 1024 in the output tile, and
the

154
00:12:24,480 --> 00:12:29,950
mask width of 5, we are going to have 1020
elements

155
00:12:29,950 --> 00:12:34,670
in the output tile.
So, that defines the output tile width.

156
00:12:34,670 --> 00:12:40,320
So, this gives us 1020 elements in the
output tile, and then

157
00:12:40,320 --> 00:12:46,159
we define each block to have 1024.

158
00:12:46,159 --> 00:12:51,630
1024 is a convenient due to the power
number which is the perfect power of two,

159
00:12:51,630 --> 00:12:57,640
so this gives us a nice block size and the
nice number of threads in the execution.

160
00:12:57,640 --> 00:13:01,250
So once we define the block width and output
tile width,

161
00:13:01,250 --> 00:13:05,805
block width is essentially the same as
input tile width.

162
00:13:05,805 --> 00:13:11,532
Then we can have, we can say the block dim
the, the dimension of block and

163
00:13:11,532 --> 00:13:15,018
dimension of grid to be block width and 1,
1 just

164
00:13:15,018 --> 00:13:19,417
like the vector addition case and then we
can have the dimension

165
00:13:19,417 --> 00:13:23,069
of the grid to be width mi, minus 1
divided by output

166
00:13:23,069 --> 00:13:28,640
width and then the, the [UNKNOWN] the
division result would add one.

167
00:13:28,640 --> 00:13:31,270
So this essentially is, is the ceiling

168
00:13:31,270 --> 00:13:35,898
calculation of width divided by output
tile width.

169
00:13:35,898 --> 00:13:36,858
So remember

170
00:13:36,858 --> 00:13:42,714
the the number of, of thread blocks we
need is determined by the

171
00:13:42,714 --> 00:13:48,880
number of thread blocks needed to
calculate all the output elements.

172
00:13:48,880 --> 00:13:52,260
So because every thread block is going to
generate a

173
00:13:52,260 --> 00:13:55,148
output tile with number of elements, we
need to

174
00:13:55,148 --> 00:13:58,300
take the the width of the output and
divide

175
00:13:58,300 --> 00:14:01,940
it by output tile width and take that
ceiling function.

176
00:14:01,940 --> 00:14:05,290
So this gives us enough threat blocks to

177
00:14:05,290 --> 00:14:09,130
calculate all the output elements, but
then we need

178
00:14:09,130 --> 00:14:12,570
to define the dimension of each block to
be

179
00:14:12,570 --> 00:14:15,890
big enough to cover all the input tile
elements.

180
00:14:15,890 --> 00:14:20,810
So this is actually a confusing part of a
tile algorithm

181
00:14:20,810 --> 00:14:25,170
where the input tiles and output tiles are
of different sizes.

182
00:14:25,170 --> 00:14:27,228
So, in this, in our

183
00:14:27,228 --> 00:14:33,990
example, when the mask width is 5 we would
have essentially output tile

184
00:14:33,990 --> 00:14:40,780
width plus 4 to be the the block to, to be
the block width.

185
00:14:40,780 --> 00:14:45,130
But in general, the, the, the block width
should be

186
00:14:45,130 --> 00:14:48,631
the output tile width plus mask width
minus 1.

187
00:14:52,580 --> 00:14:57,250
So now that you, you have a a, a high
level concept

188
00:14:57,250 --> 00:15:01,810
of how the one dimensional tiled kernel
should be

189
00:15:01,810 --> 00:15:07,190
designed, we also need to understand some
preliminaries

190
00:15:07,190 --> 00:15:12,470
in understanding the benefit of tile
algorithms for convolution patterns.

191
00:15:12,470 --> 00:15:18,150
So this slide shows that that shared data

192
00:15:18,150 --> 00:15:21,980
reuse in a in our small example.

193
00:15:21,980 --> 00:15:28,200
So whenever we load these elements into
the shared memory we would

194
00:15:28,200 --> 00:15:31,450
we would like to be able to reuse these
element multiple times.

195
00:15:32,548 --> 00:15:37,860
Unlike the, in the the matrix
multiplication example that convolution

196
00:15:37,860 --> 00:15:42,948
calculation do not have the same number of
use,

197
00:15:42,948 --> 00:15:47,150
re-use for all the input elements loaded
into the shared memory.

198
00:15:47,150 --> 00:15:53,126
So if we look at all the elements that we
loaded in the shared memory in our

199
00:15:53,126 --> 00:15:55,865
example, element 2 is only going to be

200
00:15:55,865 --> 00:16:00,310
used in the calculation of element, output
element 4.

201
00:16:00,310 --> 00:16:04,426
So you know that the calculation of
element 4

202
00:16:04,426 --> 00:16:07,954
will require 2, 3, 4, 5, 6 and then

203
00:16:07,954 --> 00:16:14,594
the calculation of element 5 is on, is
going to involve 3, 4, 5, 6, 7.

204
00:16:14,594 --> 00:16:17,668
So 2 is not going to be involved in any

205
00:16:17,668 --> 00:16:23,370
other calculation of any other output
element than 4.

206
00:16:23,370 --> 00:16:28,210
So that's why element 4 would be only used
once when

207
00:16:28,210 --> 00:16:33,750
it's loaded into the shared memory.
So the shared memory will not provide

208
00:16:33,750 --> 00:16:36,970
nice re-use for that particular element.

209
00:16:36,970 --> 00:16:41,758
However, element 3 would be re-used a
little bit more, because element

210
00:16:41,758 --> 00:16:46,790
3 is going to be used for calculating both
element 4 and element 5.

211
00:16:46,790 --> 00:16:52,655
And element 4 in the input is going to be
cal, used in the

212
00:16:52,655 --> 00:16:58,859
calculation of output 4, 5 and 6.
And input element 5 is

213
00:16:58,859 --> 00:17:05,190
going to be calculated, used in the
calculation of 4, 5, 6 and 7.

214
00:17:05,190 --> 00:17:09,710
So, if we continue this process we will
see that each

215
00:17:09,710 --> 00:17:14,380
input element loaded into the shared
memory will be reused different

216
00:17:14,380 --> 00:17:18,045
number of times and this is going to be
important in understanding

217
00:17:18,045 --> 00:17:24,090
the net benefit of loading data input
value into tile into

218
00:17:24,090 --> 00:17:28,360
the shared memory when we do the, do the
benefit analysis.

219
00:17:29,950 --> 00:17:34,090
It's also important to understand that
some of the

220
00:17:34,090 --> 00:17:37,390
data re-used will also be affected by
those cells.

221
00:17:37,390 --> 00:17:43,594
So when we, when look at the, the input
tile 0, what we see that

222
00:17:43,594 --> 00:17:49,610
for a mask width of 5 we are going to have
two ghost elements on the on

223
00:17:49,610 --> 00:17:54,744
the left hand side.
So the calculation of output val,

224
00:17:54,744 --> 00:17:59,848
element 0 and 1 will involve the the zero
the ghost element

225
00:17:59,848 --> 00:18:04,776
values and these values are assigned into
the shared memory rather

226
00:18:04,776 --> 00:18:10,370
than loaded from the gol global memory
into the shared memory.

227
00:18:10,370 --> 00:18:14,830
So this will actually effect the the the
memory number

228
00:18:14,830 --> 00:18:20,710
of memory accesses that we need to make or
the net benefit of a tiled algorithm.

229
00:18:20,710 --> 00:18:25,069
So we are going to come back to this point
later in a future lecture.

230
00:18:26,790 --> 00:18:30,080
So at this point, you'll have the basic
understanding

231
00:18:30,080 --> 00:18:36,840
of a tiled algorithm in particular, tiled
1D convolution algorithm.

232
00:18:36,840 --> 00:18:40,133
So, you, if you'd like to understand

233
00:18:40,133 --> 00:18:43,693
more about this aspect, I'd like to
encourage

234
00:18:43,693 --> 00:18:47,388
you to read section 8.4 of the textbook.

235
00:18:47,388 --> 00:18:48,631
Thank you.

