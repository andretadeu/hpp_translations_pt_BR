1
00:00:00,370 --> 00:00:01,050
Hi,

2
00:00:05,570 --> 00:00:06,280
everyone.

3
00:00:06,280 --> 00:00:09,470
Welcome back to the Heterogeneous Parallel
Programming class.

4
00:00:09,470 --> 00:00:12,500
We are at lecture 3.5 and we're going to
be

5
00:00:12,500 --> 00:00:17,670
discussing the development of a two
dimensional tiled convolution kernel.

6
00:00:17,670 --> 00:00:21,190
In the previous lecture we have already

7
00:00:21,190 --> 00:00:24,800
discussed how to write a one dimensional
tiled

8
00:00:24,800 --> 00:00:27,140
convolution kernel and in this lecture we

9
00:00:27,140 --> 00:00:31,000
are going to extend that concept into real
image

10
00:00:31,000 --> 00:00:34,700
files that usually are the application of
a

11
00:00:34,700 --> 00:00:39,030
two Dimensional Convolution and then we
would discuss some

12
00:00:39,030 --> 00:00:41,730
of the things that are more specific to
the

13
00:00:41,730 --> 00:00:48,100
image processing aspect of 2D dimensional,
two dimensional convolution.

14
00:00:48,100 --> 00:00:51,070
So the objective of this lecture is for
you to learn how to

15
00:00:51,070 --> 00:00:56,110
write a two dimensional convolution kernal
for image processing and so we are going

16
00:00:56,110 --> 00:01:01,890
to look at two dimensioanl image data
types and API functions and we'll learn

17
00:01:01,890 --> 00:01:08,800
how to use constant caching In to improved
performance for a for the mask.

18
00:01:08,800 --> 00:01:10,720
And then we're going to discuss how to

19
00:01:10,720 --> 00:01:13,830
extend the concepts of input tiles versus
output

20
00:01:13,830 --> 00:01:17,380
tiles, thread to data index mapping and
handling

21
00:01:17,380 --> 00:01:21,380
boundary conditions from one dimension to
two dimensions.

22
00:01:24,340 --> 00:01:28,300
While this slide shows a the concept of

23
00:01:28,300 --> 00:01:33,612
a typical image matrix representation in
application packages.

24
00:01:33,612 --> 00:01:40,660
So,um, we, a lot, a lot of times we will
find it desirable

25
00:01:40,660 --> 00:01:47,830
for to pad each row of a two dimensional
matrix to multiples of DRAM burst.

26
00:01:47,830 --> 00:01:49,480
So in a lot of the real

27
00:01:49,480 --> 00:01:54,410
applications the, developers often time
find

28
00:01:54,410 --> 00:01:59,430
that the desirable or advantageous

29
00:01:59,430 --> 00:02:05,350
To to you know, pad, to add padded
elements to each row.

30
00:02:05,350 --> 00:02:10,070
So that the size of each row becomes
multiples of DRAM bursts.

31
00:02:10,070 --> 00:02:15,230
And by starting the first row at the first
boundary with the padding.

32
00:02:15,230 --> 00:02:20,090
Every subsequent row will also start at
DRAM burst boundaries.

33
00:02:20,090 --> 00:02:25,000
So this essentially allows the the program
to

34
00:02:25,000 --> 00:02:30,060
be able to better control the DRAM burst
utilization in

35
00:02:30,060 --> 00:02:35,630
their data accesses.
And if we look at the padded

36
00:02:35,630 --> 00:02:40,520
matrix we, because we're adding elements
To each row

37
00:02:40,520 --> 00:02:45,492
we effectively are adding entire columns
into this matrices.

38
00:02:45,492 --> 00:02:50,598
So here is the simple example, if we
assume that we have a

39
00:02:50,598 --> 00:02:56,200
matrix of 3.3, and if the DRAM

40
00:02:56,200 --> 00:03:00,910
bursts is four elements then we can, we
need to add one

41
00:03:00,910 --> 00:03:05,820
more element or pad one more element into
each row to make it into

42
00:03:05,820 --> 00:03:12,850
a four even burst size.
So, in this case, the pitch will be four

43
00:03:12,850 --> 00:03:18,620
and then one more than that the width.
So, in this example,

44
00:03:18,620 --> 00:03:24,240
the height is 3 The width is 3 and then
the pitch is 4.

45
00:03:24,240 --> 00:03:29,150
In most of the images today are colored
images.

46
00:03:29,150 --> 00:03:31,290
That means that each pixel

47
00:03:31,290 --> 00:03:37,240
is not a single value but it's actually a
combination of colored channels.

48
00:03:37,240 --> 00:03:40,800
And that the most typical representation
of a colored

49
00:03:40,800 --> 00:03:45,630
image is R, G, and B, red green and blue.

50
00:03:45,630 --> 00:03:51,420
So, eh, each pixel will typically have
three values rather than one and

51
00:03:51,420 --> 00:03:56,320
so in this example we, we do not have
multiple values

52
00:03:56,320 --> 00:04:02,040
in each element so this, in this example
we say there's only one Channel.

53
00:04:02,040 --> 00:04:06,958
However, in the lab you will need to be
able to process

54
00:04:06,958 --> 00:04:12,540
a color image with three channels in each
pixel.

55
00:04:12,540 --> 00:04:16,770
So the padding is typically done

56
00:04:16,770 --> 00:04:21,940
automatically by matrix allocation or
matrix importing functions.

57
00:04:21,940 --> 00:04:27,210
And the implementation of these functions
will typically look at the hardware

58
00:04:27,210 --> 00:04:32,930
DRAM burst configuration and the
optionally add these padded elements.

59
00:04:32,930 --> 00:04:36,470
So so that when you call the same

60
00:04:36,470 --> 00:04:39,530
library function depending on the hardware
that you're

61
00:04:39,530 --> 00:04:43,310
running the application you might get
different padded

62
00:04:43,310 --> 00:04:47,270
padding or different pitch value when you
get

63
00:04:47,270 --> 00:04:51,730
the array matrix from these functions.
So

64
00:04:51,730 --> 00:04:56,080
this is the picture that I would like you
to remember about pitch when

65
00:04:56,080 --> 00:05:00,370
you process images.
So in the pattern we have show that here,

66
00:05:00,370 --> 00:05:06,160
there is a padded matrix, conceptually
padded matrix with one

67
00:05:06,160 --> 00:05:12,510
padded column and then we lay out the
matrix into row-major layout In

68
00:05:12,510 --> 00:05:17,870
the memory you will see that there is one
extra element at the end of every

69
00:05:17,870 --> 00:05:23,490
row in the lay out and the so these
elements are non-existing elements.

70
00:05:23,490 --> 00:05:31,820
So the important part is when we start to
use linearized elements access indices.

71
00:05:32,960 --> 00:05:37,990
We need to use the pitch rather than the
original width

72
00:05:37,990 --> 00:05:40,910
when we skip over entire rows.

73
00:05:40,910 --> 00:05:47,210
So lets say, if we want to access element
M(2,1) and then

74
00:05:47,210 --> 00:05:53,270
originallly we would multiply 2 by the
width which is 3 plus the column.

75
00:05:53,270 --> 00:05:58,320
However because of the, the padding We
actually need to multiply the row index

76
00:05:58,320 --> 00:06:03,320
by the pitch value so there was skip over
enough of these elements and

77
00:06:03,320 --> 00:06:07,550
then so that we can get to the right M
element.

78
00:06:07,550 --> 00:06:13,020
So in order to access M 2,1 we need to
multiply 2 By

79
00:06:13,020 --> 00:06:18,750
pitch value, which is 4 and then we add
one the column index.

80
00:06:18,750 --> 00:06:20,230
So this gives us 9.

81
00:06:20,230 --> 00:06:24,950
So this what lead us into the effect of
M9,

82
00:06:24,950 --> 00:06:28,350
which is actually, you know, which will
include the effect

83
00:06:28,350 --> 00:06:30,590
of the padded elements

84
00:06:30,590 --> 00:06:36,970
M3 and M7 that didn't existed in the original
matrix.

85
00:06:38,650 --> 00:06:42,690
So, now that you understand the concept of
pitch and the channels

86
00:06:44,770 --> 00:06:49,930
we show a summary of the matrix, image
matrix type that

87
00:06:49,930 --> 00:06:54,680
you will be using in the in the
heterogeneous parallel programming class.

88
00:06:54,680 --> 00:07:00,200
So this is the the data type that we will
assume for images.

89
00:07:00,200 --> 00:07:03,900
And this, you know, the this is only going

90
00:07:03,900 --> 00:07:07,420
to be using the host code of the machine
problem.

91
00:07:07,420 --> 00:07:09,940
And by the time you evoke your kernel you

92
00:07:09,940 --> 00:07:13,510
should have extracted the data and the
width and height

93
00:07:13,510 --> 00:07:18,380
and pitch, and send them into the kernel,
for the kernel to process the data.

94
00:07:18,380 --> 00:07:21,180
So when you look at the host code, it
should

95
00:07:21,180 --> 00:07:24,830
be now clear to you what the host code is
doing.

96
00:07:24,830 --> 00:07:30,180
It's actually reading a real light im-
color image from a file And then that

97
00:07:30,180 --> 00:07:35,140
it would populate the value into all these
fields in

98
00:07:35,140 --> 00:07:37,700
the image data and then the host code

99
00:07:37,700 --> 00:07:41,530
will extract these values but before it
launches

100
00:07:41,530 --> 00:07:47,660
the kernel or do the copying of the data
from host memory into the device memory.

101
00:07:48,790 --> 00:07:53,800
So here are some of the, API functions
that you will see in the host code.

102
00:07:53,800 --> 00:08:00,260
So basically, we can, allocate the new
fun, new, new da, image so this will

103
00:08:00,260 --> 00:08:05,810
be done for the output image and we could
import a image in the process we'll

104
00:08:05,810 --> 00:08:11,770
also create, allocate memory for that in,
input image so this is done by the

105
00:08:11,770 --> 00:08:16,520
wbImport function, and we also have
several

106
00:08:16,520 --> 00:08:20,470
utility functions that help us to extract
the

107
00:08:20,470 --> 00:08:23,710
relevant information from the object, so
the width

108
00:08:23,710 --> 00:08:25,630
of the image, the height of the image,

109
00:08:25,630 --> 00:08:27,980
number of channels in each pixel of the
image,

110
00:08:27,980 --> 00:08:31,640
and then we can get the pitch of the
image.

111
00:08:31,640 --> 00:08:36,920
But so all these values could be extracted
by these functions uh,and

112
00:08:36,920 --> 00:08:41,930
so that we can set up the configurations
property when we launch kernels.

113
00:08:41,930 --> 00:08:46,900
So for simplicity the pitch of all the
matrices in

114
00:08:46,900 --> 00:08:51,240
this lap being in that, in the being our
lap

115
00:08:51,240 --> 00:08:54,630
Will be set to width times channels.

116
00:08:54,630 --> 00:08:59,700
So, which means that there's no just no
padded elements.

117
00:08:59,700 --> 00:09:03,320
So, now after we talk about all these
concepts, we'll end up

118
00:09:03,320 --> 00:09:08,400
using width the width value for the pitch
value in all the cases.

119
00:09:08,400 --> 00:09:13,670
So, this allows you to have a simpler way
of writing your kernel.

120
00:09:13,670 --> 00:09:16,520
However, I did want to make sure that you
understand

121
00:09:16,520 --> 00:09:20,100
the concept so that you can easily and
quickly adapt

122
00:09:20,100 --> 00:09:23,600
the kernel function you write for this
course to handle

123
00:09:23,600 --> 00:09:28,050
real life image cases where there might be
a pitch

124
00:09:28,050 --> 00:09:30,890
you know a value that is different from
the width.

125
00:09:30,890 --> 00:09:34,830
So, the, the use of all these API
functions have

126
00:09:34,830 --> 00:09:38,520
been, has been done in the host code given
to you.

127
00:09:38,520 --> 00:09:41,920
So you don't need to understand the, the
intricate

128
00:09:41,920 --> 00:09:44,660
details of all these function definitions.

129
00:09:44,660 --> 00:09:47,610
However, I do want to make sure that I
mention these

130
00:09:47,610 --> 00:09:50,900
functions to you so that when you read the
host code.

131
00:09:50,900 --> 00:09:53,320
You would be able to more confidently

132
00:09:53,320 --> 00:09:56,070
understand what's going on in the host
code.

133
00:09:58,160 --> 00:10:01,340
So now we are ready to extend the concepts
that we

134
00:10:01,340 --> 00:10:07,420
learned in the 1D kernel case into a 2D
convolution kernel.

135
00:10:07,420 --> 00:10:11,070
So uh,we still have the concept of output
tile width.

136
00:10:11,070 --> 00:10:17,740
And in this case, we assume that the 2D
case we assume a two,

137
00:10:17,740 --> 00:10:23,160
square two dimensional tile and the upper
tile is still going to be smaller than the

138
00:10:23,160 --> 00:10:28,870
input tile and the the thread blocks are
going to be configured to cover

139
00:10:28,870 --> 00:10:34,670
all the input tile elements.
So by assuming a square

140
00:10:34,670 --> 00:10:40,660
tile we we were going, we're going to de,
decline define a Block Width

141
00:10:40,660 --> 00:10:46,320
which is mask mu, width -1 bigger than the
output tile size.

142
00:10:46,320 --> 00:10:48,680
So that essentially we get the

143
00:10:48,680 --> 00:10:50,040
extra element.

144
00:10:50,040 --> 00:10:54,100
In the in the input tile that can that

145
00:10:54,100 --> 00:10:57,260
are needed to generate and the entire
upper tile.

146
00:10:57,260 --> 00:11:00,030
So, in this case, we are extending both

147
00:11:00,030 --> 00:11:03,600
in the horizontal direction and the
vertical direction

148
00:11:03,600 --> 00:11:06,400
to be able to to gather all the

149
00:11:06,400 --> 00:11:10,500
input elements needed to generate a square
output tile.

150
00:11:10,500 --> 00:11:14,550
And then, so when we see setup the, the
block Thread blocks.

151
00:11:14,550 --> 00:11:20,230
We are going to be using the input the
input tile size which is the block

152
00:11:20,230 --> 00:11:26,030
width here is you know, mask width - 1
bigger than alpha tile width.

153
00:11:26,030 --> 00:11:29,190
And then, here, we assume that the mask
size is 5.

154
00:11:29,190 --> 00:11:34,250
And then, in general we should, we should
just add mask width

155
00:11:35,300 --> 00:11:40,590
minus 1 to whatever output tile width we
have, to get the block size.

156
00:11:40,590 --> 00:11:44,160
And then you know, we're going to take the
width of

157
00:11:44,160 --> 00:11:48,030
the image and the height of the image, and
divide it by

158
00:11:48,030 --> 00:11:51,780
the output width, tile width, and take the
ceiling function, to make

159
00:11:51,780 --> 00:11:56,360
sure we have enough thread blocks to
generate All the output elements.

160
00:11:56,360 --> 00:12:01,730
So, here we assume that the input image
and output image are of the

161
00:12:01,730 --> 00:12:06,870
same width and height, which is usually
the case for convolution computation.

162
00:12:08,860 --> 00:12:14,340
So, now u, we have an opportunity to
introduce constant memory

163
00:12:14,340 --> 00:12:19,120
and constant caching in CUDA.
So if

164
00:12:19,120 --> 00:12:24,000
we look at the access pattern for mask,
for the convolution mask, the

165
00:12:24,000 --> 00:12:29,110
mask is used by all the threads.
And but it's not,

166
00:12:29,110 --> 00:12:33,910
not modified in the convolution kernel,
and all the threads

167
00:12:33,910 --> 00:12:39,280
in the warp will access the same locations
of the mask, at each point in time.

168
00:12:39,280 --> 00:12:41,330
So, if we look at all the 32 threads

169
00:12:41,330 --> 00:12:44,490
in the warp, All those threads will be
accessing element

170
00:12:44,490 --> 00:12:47,140
zero at the same time and then they would
access

171
00:12:47,140 --> 00:12:51,180
Mask array element one at the same time
and so on.

172
00:12:51,180 --> 00:12:55,070
So this is very similar to a situation
where if

173
00:12:55,070 --> 00:12:59,450
we had a program constant or coefficient
in our statement

174
00:12:59,450 --> 00:13:02,320
then all the threads executing that
statement

175
00:13:02,320 --> 00:13:04,440
Is going to access the same value.

176
00:13:04,440 --> 00:13:07,880
So this is the origin of the name constant
memory.

177
00:13:07,880 --> 00:13:12,970
We are storing all those values in a
memory that will be aggressively cached.

178
00:13:12,970 --> 00:13:17,850
And then the access value from the cache
will be broadcast to all the threads in a

179
00:13:17,850 --> 00:13:23,630
warp to, you know, even more drastically
magnify the memory bandwidth.

180
00:13:23,630 --> 00:13:24,590
When,

181
00:13:24,590 --> 00:13:26,170
by that cache.

182
00:13:26,170 --> 00:13:31,709
So, in the newer CUDA generation, if we
use const, and,

183
00:13:31,709 --> 00:13:37,800
_ _,, restrict, _,_, qualifiers in front
of a,

184
00:13:37,800 --> 00:13:42,150
a parameter pointer, then the compiler,
will be able to

185
00:13:42,150 --> 00:13:47,240
know that this particular data structure
is eligible for constant caching.

186
00:13:47,240 --> 00:13:49,970
So here is a [INAUDIBLE] a simple example
you

187
00:13:49,970 --> 00:13:54,790
know, you can potentially write a, a
kernel function and then the

188
00:13:54,790 --> 00:13:59,940
kernel function will receive the data
portions of the of the input and

189
00:13:59,940 --> 00:14:03,080
the height of the image, the width of the
image, that bombards channel

190
00:14:03,080 --> 00:14:07,990
in the image and then then you'll also
need to receive the mask.

191
00:14:07,990 --> 00:14:11,180
So the mask Is actually a parameter that
needs

192
00:14:11,180 --> 00:14:15,716
to be preceded by const and _, _ restrict

193
00:14:15,716 --> 00:14:20,210
_,_ qualifier, so that the compiler knows

194
00:14:20,210 --> 00:14:24,900
that the mask elements are eligible for
constant caching.

195
00:14:26,710 --> 00:14:31,360
So here we extend the concept of input
tiles and output

196
00:14:31,360 --> 00:14:35,930
tiles into two dimensions.
So here we

197
00:14:35,930 --> 00:14:40,720
show that when we assign the elem, input
element and

198
00:14:40,720 --> 00:14:46,080
output elements to each thread.
We are going to shift the the

199
00:14:46,080 --> 00:14:50,460
output index by 2 to the left and also the

200
00:14:50,460 --> 00:14:55,580
output column index by 2 to the to

201
00:14:55,580 --> 00:15:00,840
to above so that essentially we move each,

202
00:15:00,840 --> 00:15:05,780
the thread 0 will be responsible for the
corner input tile

203
00:15:05,780 --> 00:15:11,990
element and it will also be responsible
for the corner output tile element.

204
00:15:11,990 --> 00:15:14,410
So this is just a straightforward you

205
00:15:14,410 --> 00:15:17,600
know, extension of the one dimensional
case.

206
00:15:17,600 --> 00:15:21,200
So, this allows all the threads from 0 to

207
00:15:21,200 --> 00:15:25,270
output thread block, to generate all the
output elements.

208
00:15:25,270 --> 00:15:31,520
And then, we have more of the elements to
participate in loading the input elements.

209
00:15:31,520 --> 00:15:35,810
So, this leads us into the loading of the
input element.

210
00:15:35,810 --> 00:15:40,110
This is also a very straightforward
expansion of the two-dimensional case.

211
00:15:40,110 --> 00:15:42,050
Where we not only need to test

212
00:15:42,050 --> 00:15:46,610
our horizontal which is this col_icase but
also

213
00:15:46,610 --> 00:15:51,660
we need to make sure that that the row
indices are also within the valid range.

214
00:15:51,660 --> 00:15:57,609
So whenever a load of the input value is
within the valid

215
00:15:57,609 --> 00:15:59,156
range then we load that into

216
00:15:59,156 --> 00:16:02,670
a two-dimensional shared memory
[INAUDIBLE] array.

217
00:16:02,670 --> 00:16:08,350
Indexed by the squared index y and x
dimension of the thread index.

218
00:16:08,350 --> 00:16:10,770
And, whenever this offset is out of reach,

219
00:16:10,770 --> 00:16:12,950
then we know that it's loading a ghost
element,

220
00:16:12,950 --> 00:16:15,520
so in this particular case we still assume
we

221
00:16:15,520 --> 00:16:19,420
have the policy of setting those elements
to 0's.

222
00:16:19,420 --> 00:16:23,200
So here we look at this code.

223
00:16:23,200 --> 00:16:29,000
We have a linearized input access.
And the, if the pitch

224
00:16:29,000 --> 00:16:35,350
is different from width we should use the
pitch value in this modification.

225
00:16:35,350 --> 00:16:37,310
However, because the pitch and width are
the

226
00:16:37,310 --> 00:16:39,870
same for our labs, you can just use width.

227
00:16:39,870 --> 00:16:44,690
So whenever you need to extend this kernel
to a more general image processing

228
00:16:44,690 --> 00:16:48,380
kernel then you need to use the pitch
value and you need to supply

229
00:16:48,380 --> 00:16:52,350
that pitch value into the kernel for
appropriate access.

230
00:16:55,150 --> 00:17:01,300
And, whenever, the, once we finish noting,
remember you need to use the barrier

231
00:17:01,300 --> 00:17:06,800
synchronization be, in between the, these,
two, the loading and the computation.

232
00:17:06,800 --> 00:17:09,100
So when we enter the computation, just
like

233
00:17:09,100 --> 00:17:13,730
the one-dimensional case, the, when, only
the threads.

234
00:17:13,730 --> 00:17:18,970
Within the output tile range should
participate in the computation.

235
00:17:18,970 --> 00:17:20,290
And then so

236
00:17:20,290 --> 00:17:23,210
we're going to not only test the the x

237
00:17:24,510 --> 00:17:27,660
thread index but also the, the y thread
index.

238
00:17:27,660 --> 00:17:30,610
Otherwise, it's a straightforward
extension of

239
00:17:30,610 --> 00:17:32,370
adding another loop, so that we

240
00:17:32,370 --> 00:17:37,110
would do a two-dimensional access to the
shared memory and a mask.

241
00:17:37,110 --> 00:17:40,070
And gener accumulate into the output.

242
00:17:43,190 --> 00:17:45,800
At the end of the kernel we're going to
write output.

243
00:17:45,800 --> 00:17:51,060
And we're going to need to test whether
the the row index

244
00:17:51,060 --> 00:17:56,690
and the column index of the output index are
within the the valid range.

245
00:17:56,690 --> 00:17:58,960
So this makes sure that we don't end

246
00:17:58,960 --> 00:18:02,910
up writing into non-existing work or, or
ghost elements.

247
00:18:02,910 --> 00:18:09,020
So this gives us the the expression for
writing the output.

248
00:18:09,020 --> 00:18:13,756
So in so far we have discussed the code in

249
00:18:13,756 --> 00:18:18,880
terms of a single channel pixel format.

250
00:18:18,880 --> 00:18:23,620
And you need to look at the machine
problem description on

251
00:18:23,620 --> 00:18:29,540
the GPU web GPU to see some

252
00:18:29,540 --> 00:18:34,810
instruction about how to adapt this kind
of code to accommodate

253
00:18:34,810 --> 00:18:40,090
a multi-channel pixel And you also need to
know that

254
00:18:40,090 --> 00:18:43,130
the width need to be adapted to a pitch
value if

255
00:18:43,130 --> 00:18:48,530
you are handling a more general image file
.So this

256
00:18:48,530 --> 00:18:54,670
concludes the dis, the discussion of a two
dimensional convolution kernel.

257
00:18:54,670 --> 00:18:57,940
At this point, you are ready to do the
machine problem.

258
00:18:57,940 --> 00:19:00,500
For two-dimensional convolution and

259
00:19:00,500 --> 00:19:02,710
then for those of you who would like to

260
00:19:02,710 --> 00:19:07,910
learn more about constant memory usage or
convolution in general.

261
00:19:07,910 --> 00:19:14,904
I'd like to encourage to read section 8.3
of the textbook.Thank you.

